{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter: num_epochs\n",
    "\n",
    "Parameters          | 5th Train-Run | 6th Train-Run |\n",
    "----------          | ------------- | ------------- |\n",
    "sequence_length     |   5           |   5           |  \n",
    "batch_size          |   128         |   128         |   \n",
    "dropout (LSTM)      |   0.5         |   0.5         |  \n",
    "dropout (Layer)     |   no dropout  |   no dropout  |   \n",
    "num_epochs          |   **20**      |   **5**        |   \n",
    "learning_rate       |   0.001       |   0.001       |\n",
    "vocab_size          |   vocab_size  |   vocab_size  |   \n",
    "output_size         |   vocab_size  |   vocab_size  |   \n",
    "embedding_dim       |   200         |   200         |  \n",
    "hidden_dim          |   256         |   256         |  \n",
    "n_layers            |   2           |   2           |  \n",
    "show_every_n_batches|   500         |   500         |   \n",
    "**Loss**            |   **3.26**    |   **3.65**     \n",
    "\n",
    "### Parameter: learning_rate\n",
    "\n",
    "Parameters          | 2nd Train-Run | 3rd Train-Run |\n",
    "----------          | ------------- | ------------- |\n",
    "sequence_length     |   5           |   5           |  \n",
    "batch_size          |   128         |   128         |   \n",
    "dropout (LSTM)      |   0.5         |   0.5         |   \n",
    "dropout (Layer)     |   0.5         |   0.5         |   \n",
    "num_epochs          |   5           |   5           |  \n",
    "learning_rate       |   **0.001**     |   **0.01**      |   \n",
    "vocab_size          |   vocab_size  | vocab_size    |   \n",
    "output_size         |   vocab_size  |   vocab_size  |  \n",
    "embedding_dim       |   400         |   400         |  \n",
    "hidden_dim          |   512         |   512         |   \n",
    "n_layers            |   2           |   2           |   \n",
    "show_every_n_batches|   500         |   500         |   \n",
    "**Loss**            |   **3.96**    |   **4.89**    |   \n",
    "\n",
    "### Parameter: batch_size\n",
    "\n",
    "Parameters          | 7th Train-Run | 6th Train-Run | 8th Train-Run | 9th Train-Run |\n",
    "----------          | ------------- | ------------- | ------------- | ------------- |\n",
    "sequence_length     |   5           |   5           |   5           |   5      \n",
    "batch_size          |   **64**        |   **128**       |   **256**       |   **512**\n",
    "dropout (LSTM)      |   0.5         |   0.5         |   0.5         |   0.5     \n",
    "dropout (Layer)     |   no dropout  |   no dropout  |   no dropout  |   no dropout  \n",
    "num_epochs          |   5           |   5           |   5           |   5     \n",
    "learning_rate       |   0.001       |   0.001       |   0.001       |   0.001\n",
    "vocab_size          |   vocab_size  |   vocab_size  |   vocab_size  |   vocab_size\n",
    "output_size         |   vocab_size  |   vocab_size  |   vocab_size  |   vocab_size\n",
    "embedding_dim       |   200         |   200         |   200         |   200\n",
    "hidden_dim          |   256         |   256         |   256         |   256\n",
    "n_layers            |   2           |   2           |   2           |   2\n",
    "show_every_n_batches|   500         |   500         |   500         |   500\n",
    "**Loss**            |   **3.81**    |   **3.65**    |   **3.63**    |   **3.74**\n",
    "\n",
    "### Parameter: embedding_dim\n",
    "\n",
    "Parameters          | 8th Train-Run | 10th Train-Run | 11th Train-Run |\n",
    "----------          | ------------- | -------------  | -------------  |\n",
    "sequence_length     |   5           |   5            |  5             |\n",
    "batch_size          |   256         |   256          |  256           |\n",
    "dropout (LSTM)      |   0.5         |   0.5          |  0.5\n",
    "dropout (Layer)     |   no dropout  |   no dropout   |  no dropout\n",
    "num_epochs          |   5           |   5            |  5\n",
    "learning_rate       |   0.001       |   0.001        |  0.001\n",
    "vocab_size          |   vocab_size  |   vocab_size   |  vocab_size\n",
    "output_size         |   vocab_size  |   vocab_size   |  vocab_size\n",
    "embedding_dim       |   **200**       |   **400**        |  **600**         |\n",
    "hidden_dim          |   256         |   256          |  256           |\n",
    "n_layers            |   2           |   2            |  2\n",
    "show_every_n_batches|   500         |   500          |  500\n",
    "**Loss**            |   **3.63**    |   **3.58**     |  **3.60**\n",
    "\n",
    "### Parameter: hidden_dim\n",
    "\n",
    "Parameters          | 13th Train-Run| 8th Train-Run  | 12th Train-Run|\n",
    "----------          | ------------- | -------------  | -------------  |\n",
    "sequence_length     |   5           |   5            |  5             |\n",
    "batch_size          |   256         |   256          |  256           |\n",
    "dropout (LSTM)      |   0.5         |   0.5          |  0.5\n",
    "dropout (Layer)     |   no dropout  |   no dropout   |  no dropout\n",
    "num_epochs          |   5           |   5            |  5\n",
    "learning_rate       |   0.001       |   0.001        |  0.001\n",
    "vocab_size          |   vocab_size  |   vocab_size   |  vocab_size\n",
    "output_size         |   vocab_size  |   vocab_size   |  vocab_size\n",
    "embedding_dim       |   200         |   200          |  200           |\n",
    "hidden_dim          |   **128**       |   **256**        |  **512**\n",
    "n_layers            |   2           |   2            |  2\n",
    "show_every_n_batches|   500         |   500          |  500\n",
    "**Loss**            |   **3.80**    |   **3.63**     |  **3.34**\n",
    "\n",
    "### Parameter: sequence_length\n",
    "\n",
    "Parameters          | 8th Train-Run | 14th Train-Run | 15th Train-Run|\n",
    "----------          | ------------- | -------------  | -------------  |\n",
    "sequence_length     |   **5**         |   **10**         |  **15**          |\n",
    "batch_size          |   256         |   256          |  256           |\n",
    "dropout (LSTM)      |   0.5         |   0.5          |  0.5\n",
    "dropout (Layer)     |   no dropout  |   no dropout   |  no dropout\n",
    "num_epochs          |   5           |   5            |  5\n",
    "learning_rate       |   0.001       |   0.001        |  0.001\n",
    "vocab_size          |   vocab_size  |   vocab_size   |  vocab_size\n",
    "output_size         |   vocab_size  |   vocab_size   |  vocab_size\n",
    "embedding_dim       |   200         |   200          |  200           |\n",
    "hidden_dim          |   256         |   256          |  256\n",
    "n_layers            |   2           |   2            |  2\n",
    "show_every_n_batches|   500         |   500          |  500\n",
    "**Loss**            |   **3.63**    |   **3.61**       |  **3.59**\n",
    "\n",
    "### Influence of an additional Dropout layer\n",
    "\n",
    "Parameters          | 4th Train-Run | 5th Train-Run |\n",
    "----------          | ------------- | ------------- |\n",
    "sequence_length     |   10          |   10          |   \n",
    "batch_size          |   128         |   128         |   \n",
    "dropout (LSTM)      |   0.5         |   0.5         |  \n",
    "dropout (Layer)     |   **0.3**       |   **no dropout**|   \n",
    "num_epochs          |   20          |   20          |  \n",
    "learning_rate       |   0.001       |   0.001       |   \n",
    "vocab_size          |   vocab_size  |   vocab_size  |  \n",
    "output_size         |   vocab_size  |   vocab_size  |   \n",
    "embedding_dim       |   200         |   200         |   \n",
    "hidden_dim          |   256         |   256         |   \n",
    "n_layers            |   2           |   2           |  \n",
    "show_every_n_batches|   500         |   500         |   \n",
    "**Loss**            |   **3.65**    |   **3.26**    |   \n",
    "\n",
    "The influence of an additional Dropout Layer before the Fully Connected Layer can be studied between the 4th and 5th Train-Run. In the 4th Train-Run an additional Dropout-Layer with a Dropout Probability of 0.3 was used. For the 5th Train-Run the same parameter set as for the 4th Train-Run was used, however this additional Dropout layer was left out. As it can be seen this additional Dropout layer does not lead to an improvement in performance, as the loss increases.\n",
    "\n",
    "\n",
    "### Tested Best-of hyperparameter setting\n",
    "\n",
    "Parameters          | 16th Train-Run |\n",
    "----------          | ------------- |\n",
    "sequence_length     |   15          |   \n",
    "batch_size          |   256         |\n",
    "dropout (LSTM)      |   0.5         |   \n",
    "dropout (Layer)     |   no dropout  |\n",
    "num_epochs          |   20          |  \n",
    "learning_rate       |   0.001       |   \n",
    "vocab_size          |   vocab_size  |   \n",
    "output_size         |   vocab_size  |   \n",
    "embedding_dim       |   400         |   \n",
    "hidden_dim          |   512         |  \n",
    "n_layers            |   2           |   \n",
    "show_every_n_batches|   500         |  \n",
    "**Loss**            |   **3**     |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons for the chosen hyperparameter setting:\n",
    "- A sequence_length of 15 was chosen accordingly to the number of words in typical dialog sentences.  \n",
    "- A batch_size of 256 was chosen based on a trade-off of\n",
    "    - speed up training (getting better for higher batch_sizes), \n",
    "    - no out-of-memory errors (getting worse for higher batch_sizes)\n",
    "    - good training loss  \n",
    "- Leave out additional dropout layers: An additional dropout layer right before the fully connected layer increased the training loss significantly form (3.26 to 3.65).\n",
    "- An increase of the num_epochs value reduces the training error. For num_epochs=20 a training loss goal of <3.5 was achieved.\n",
    "- The embedding_dim (see table Parameter: embedding_dim) in the tested regime between 200 and 600 had no strong influence on the training loss. Hence an embedding_dim of 400 was arbitrarily chosen.\n",
    "- Increasing the hidden_dim from 128 to 512 showed a significant influence on the training loss (see table Parameter: hidden_dim). The training loss decreased from 3.80 down to 3.34. Hence for the final test a hidden_dim of 512 was chosen.\n",
    "- Two LSTM layer were stacked as typical values for LSTM stacking are 2 or 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
